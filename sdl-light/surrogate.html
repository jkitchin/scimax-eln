<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2024-07-10 Wed 15:20 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>DOE vs active learning and surrogate models of the SDL-Light</title>
<meta name="generator" content="Org Mode" />
<style type="text/css">
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
</head>
<body>
<div id="content" class="content">
<h1 class="title">DOE vs active learning and surrogate models of the SDL-Light</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orge180823">1. Introduction</a>
<ul>
<li><a href="#org3fc7819">1.1. Optimization with a surrogate model</a></li>
</ul>
</li>
<li><a href="#orgb9add13">2. An actively learned Gaussian process surrogate</a></li>
</ul>
</div>
</div>
<p>
See <a href="./surrogate-take2.html">./surrogate-take2.html</a> for an updated version.
</p>
<div id="outline-container-orge180823" class="outline-2">
<h2 id="orge180823"><span class="section-number-2">1.</span> Introduction</h2>
<div class="outline-text-2" id="text-1">
<p>
One of the reasons we use design of experiments is to fit a surrogate model. That model may be a linear, polynomial surface model, or it could be a physics-based model. Either way, you need at least as many experiments as parameters before you can fit anything. In the case of a second-order surface response with 3 components, you have at least 10 parameters to fit, so you have to run those experiments before you can even start.
</p>

<p>
If you use an ML model instead, you can employ active learning which may result in a solution with fewer experiments. This is not guaranteed though and it depends on the nature of the model you choose.
</p>

<p>
As before, we start by setting up the measurement code.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python">import numpy as np
import matplotlib.pyplot as plt
from self_driving_lab_demo import (get_paho_client, mqtt_observe_sensor_data)

PICO_ID = 'test'
client = get_paho_client(f"sdl-demo/picow/{PICO_ID}/as7341/")

from pycse.hashcache import HashCache

@HashCache
def get_results(R, G, B, label=None):
    return mqtt_observe_sensor_data(R, G, B, pico_id=PICO_ID, client=client)
</pre>
</div>
</div>
<div id="outline-container-org3fc7819" class="outline-3">
<h3 id="org3fc7819"><span class="section-number-3">1.1.</span> Optimization with a surrogate model</h3>
<div class="outline-text-3" id="text-1-1">
<p>
I create a custom estimator for linear regression with uncertainty quantification here. I use the features I wrote in <code>pycse</code> for this, most notably the regress and predict functions. Then I make a Pipeline that generates the quadratic polynomial surface features and regresses them.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python">from pycse import regress
from pycse import predict as _predict
from sklearn.base import BaseEstimator, RegressorMixin

class LR(BaseEstimator, RegressorMixin):

    def fit(self, X, y):
        self.xtrain = np.array(X)
        self.ytrain = np.array(y)
        self.coefs_, _, _ = regress(X, y, rcond=None)
        return self

    def predict(self, X, return_std=False):
        y, _, se = _predict(self.xtrain, self.ytrain, self.coefs_, X)
        if return_std:
            return y, se
        else:
            return y

from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline

model = Pipeline([('poly', PolynomialFeatures()),
                  ('LR', LR())])
</pre>
</div>

<p>
This is the most important block. Here I introduce a new concept called the <code>Surrogate</code> decorator. This decorator wraps the function with a surrogate model. When you call the function the wrapper tries to use the surrogate. If the surrogate is not accurate enough then the true function is used and the surrogate is retrained.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python">from pycse.pyroxy import Surrogate
from tqdm import tqdm

@Surrogate(model=model, tol=100) 
def measure(RGB, label=None):
    results = []
    for rgb in tqdm(RGB):
        result = get_results(*rgb, label)
        results += [[result['ch620'], result['ch510'], result['ch470']]]
    return np.array(results)
</pre>
</div>

<p>
The surrogate we have defined needs a minimal amount of data to get started. You probably should have at least as many data points as parameters to start with plus some or you may get division by zero errors with degrees of freedom, or poorly conditioned Hessians. I initialize it here.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python">import numpy as np
np.random.seed(42)

r = measure(np.random.randint(25, 75, (12, 3)))
</pre>
</div>

<p>
Once we have initialized the model, we can try an optimization. 
</p>

<p>
I found it helpful to put bounds in to avoid going out of bounds. I think because I use integer inputs, it was also necessary to use Nelder-Mead.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python">def objective(RGB):
    RGB = np.array(RGB)
    res = measure(np.atleast_2d(RGB.astype(int)))
    return np.sum(np.abs(res - 10000))

from scipy.optimize import minimize

sol = minimize(objective, np.array([55, 45, 45]),
               bounds=[[25, 75],
                       [25, 75],
                       [25, 75]],
               method='Nelder-mead')

print(sol)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-jupyter-python">sol.x.astype(int)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-jupyter-python">print(measure([sol.x.astype(int)]))
</pre>
</div>


<div class="org-src-container">
<pre class="src src-jupyter-python">print(model.predict([sol.x.astype(int)], return_std=True))
</pre>
</div>


<div class="org-src-container">
<pre class="src src-jupyter-python">print(measure)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-jupyter-python">measure.plot();
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-orgb9add13" class="outline-2">
<h2 id="orgb9add13"><span class="section-number-2">2.</span> An actively learned Gaussian process surrogate</h2>
<div class="outline-text-2" id="text-2">
<div class="org-src-container">
<pre class="src src-jupyter-python">import numpy as np
import matplotlib.pyplot as plt
from self_driving_lab_demo import (get_paho_client, mqtt_observe_sensor_data)

PICO_ID = 'test'
client = get_paho_client(f"sdl-demo/picow/{PICO_ID}/as7341/")

def get_results(R, G, B, label=None):
    return mqtt_observe_sensor_data(R, G, B, pico_id=PICO_ID, client=client)
</pre>
</div>

<p>
I use a linear kernel here, because I know the output is practically linear in the inputs and we add a WhiteKernel to account for noise we know is present. This choice is probably important; I am injecting knowledge into the model.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python">from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel

kernel = DotProduct() + WhiteKernel(noise_level_bounds=(5, 20))
gpr = GaussianProcessRegressor(kernel=kernel, random_state=0)

from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline

model = Pipeline([('poly', PolynomialFeatures()),
                  ('gpr', gpr)])

from tqdm import tqdm
from pycse.pyroxy import Surrogate

@Surrogate(model=model, tol=50, verbose=True)
def measure(RGB, label=None):
    results = []
    for rgb in tqdm(RGB):
        result = get_results(*rgb, label)
        results += [[ result['ch620'], result['ch510'], result['ch470']]]
    return np.array(results)
</pre>
</div>

<p>
I think it is a good idea to initialize the model. Here we look at 4 points that span the space. It is not comprehensive, just enough to get some points.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python">measure([[25, 25, 25],
         [75, 75, 75],
         [25, 75, 75],
         [75, 75, 25],
         [75, 25, 75]])
</pre>
</div>

<p>
We define an objective.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python">def objective(RGB):
    RGB = np.array(RGB).astype(int)
    result = measure(np.atleast_2d(RGB))
    return np.sum(np.abs(result - [10000, 10000, 10000]))
</pre>
</div>




<p>
We try the <code>brute</code> global optimization algorithm here. I suppress warnings because I don't want to see the GPR fitting warnings on every iteration.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python">import warnings
warnings.filterwarnings("ignore")
np.random.seed(42)

from scipy.optimize import brute

sol = brute(objective, 
            ranges=[[25, 75],
                    [25, 75],
                    [25, 75]])

print(sol)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-jupyter-python">measure([sol.astype(int)])
</pre>
</div>

<div class="org-src-container">
<pre class="src src-jupyter-python">model.predict([sol.astype(int)], return_std=True)
</pre>
</div>

<p>
The true values here are not close to the Surrogate; the surrogate seems over-confident. There is not currently a way to change that.
</p>
</div>
<ol class="org-ol">
<li><a id="org9abf8e7"></a><span class="done DONE">DONE</span> add way to update surrogate when you find it is over-confident<br />
<div class="outline-text-16" id="text-2-0-0-0-0-0-0-0-0-0-0-0-0-0-1">
<p>
probably also should add some random checks with true functions.
</p>
</div>
</li>
<li><a id="org3ef114e"></a>END<br />
<div class="outline-text-16" id="text-2-0-0-0-0-0-0-0-0-0-0-0-0-0-2">
<div class="org-src-container">
<pre class="src src-jupyter-python">measure.func([sol.astype(int)])
</pre>
</div>

<div class="org-src-container">
<pre class="src src-jupyter-python">measure.plot();
</pre>
</div>

<div class="org-src-container">
<pre class="src src-jupyter-python">print(measure)
</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Created: 2024-07-10 Wed 15:20</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
