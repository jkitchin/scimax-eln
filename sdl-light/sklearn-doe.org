#+title: An sklearn-doe package with application to the SDL-Light

#+attr_org: :width 800
[[./screenshots/date-08-07-2024-time-12-03-07.png]]


In previous efforts ([[./doe.org]] and [[./anova.org]]) we manually setup a design of experiments. Here I build on those works with a new sklearn-compatible library for a surface response design.

* Setup the design

The main work is done by a pyDOE3 function to set up the design. This outputs a design in randomized order. This is the order you should run the experiments.

#+BEGIN_SRC jupyter-python 
from pycse.sklearn.surface_response import SurfaceResponse

sr = SurfaceResponse(inputs=['R', 'G', 'B'],
                     outputs=['Ro', 'Go', 'Bo'],
                     bounds=[[25, 75],
                             [25, 75],
                             [25, 75]])

input = sr.design()
input
#+END_SRC

#+RESULTS:
:RESULTS:
|    | R    | G    | B    |
|----+------+------+------|
| 8  | 50.0 | 25.0 | 25.0 |
| 5  | 75.0 | 50.0 | 25.0 |
| 11 | 50.0 | 75.0 | 75.0 |
| 9  | 50.0 | 75.0 | 25.0 |
| 12 | 50.0 | 50.0 | 50.0 |
| 14 | 50.0 | 50.0 | 50.0 |
| 2  | 25.0 | 75.0 | 50.0 |
| 0  | 25.0 | 25.0 | 50.0 |
| 3  | 75.0 | 75.0 | 50.0 |
| 4  | 25.0 | 50.0 | 25.0 |
| 10 | 50.0 | 25.0 | 75.0 |
| 6  | 25.0 | 50.0 | 75.0 |
| 13 | 50.0 | 50.0 | 50.0 |
| 1  | 75.0 | 25.0 | 50.0 |
| 7  | 75.0 | 50.0 | 75.0 |
:END:


** Run the experiments

Each row in that input array is an experiment we have to run. This is the measurement code.

#+BEGIN_SRC jupyter-python 
import numpy as np
import matplotlib.pyplot as plt
from self_driving_lab_demo import (get_paho_client, mqtt_observe_sensor_data)

PICO_ID = 'test'
client = get_paho_client(f"sdl-demo/picow/{PICO_ID}/as7341/")

from pycse.hashcache import HashCache

@HashCache
def get_results(R, G, B, label=None):
    return mqtt_observe_sensor_data(R, G, B, pico_id=PICO_ID, client=client)

def measure(R, G, B, label=None):
    results = get_results(R, G, B, label)
    return results['ch620'], results['ch510'], results['ch470']
#+END_SRC

#+RESULTS:

I iterate on each row and make a "measurement". I then convert the results to a DataFrame. The output dataframe should use the column names.

#+BEGIN_SRC jupyter-python :async yes
import pandas as pd
from tqdm import tqdm

output = []
for i, RGB in tqdm(input.iterrows()):
    result = measure(*RGB, f'jul-6-{i}')    
    output += [result]

output = sr.set_output(output)
output
#+END_SRC

#+RESULTS:
:RESULTS:
: 15it [00:00, 1484.08it/s]
|    | Ro    | Go    | Bo    |
|----+-------+-------+-------|
| 8  | 12054 | 3136  | 5236  |
| 5  | 19895 | 8182  | 7283  |
| 11 | 12375 | 13654 | 19644 |
| 9  | 12076 | 13028 | 9173  |
| 12 | 12042 | 8359  | 12395 |
| 14 | 12043 | 8362  | 12397 |
| 2  | 4564  | 13298 | 14386 |
| 0  | 4313  | 3333  | 10431 |
| 3  | 19878 | 13409 | 14449 |
| 4  | 4233  | 7999  | 7140  |
| 10 | 12333 | 3826  | 15817 |
| 6  | 4659  | 8665  | 17717 |
| 13 | 12041 | 8360  | 12395 |
| 1  | 20053 | 3621  | 10606 |
| 7  | 20036 | 8852  | 17782 |
:END:

** Doing the analysis

We use the sklearn API to ~.fit~ the results.

#+BEGIN_SRC jupyter-python  
sr.fit()
print(sr.score())
sr.parity();
#+END_SRC

#+RESULTS:
:RESULTS:
: 0.9999990139144522
[[./.ob-jupyter/69fd97aa68deb9d55f208a5483ea9778c7b9a3b8.png]]
:END:

That looks pretty good.

The class provides a summary report.

#+BEGIN_SRC jupyter-python
print(sr.summary())
#+END_SRC

#+RESULTS:
#+begin_example
15 data points
  score: 0.9999990139144522
  mae  = [1.53, 3.39, 3.44]

  rmse = [4.22, 19.7, 22.1]

Output_0 = Ro
| var   |      value |       ci_lower |      ci_upper |          se |   significant |
|-------+------------+----------------+---------------+-------------+---------------|
| 1_0   | -3511      | -3560.21       | -3461.79      | 17.724      |             1 |
| R_0   |   312.185  |   311.181      |   313.189     |  0.361548   |             1 |
| G_0   |    -4.28   |    -5.28382    |    -3.27618   |  0.361548   |             1 |
| B_0   |    -2.675  |    -3.67882    |    -1.67118   |  0.361548   |             1 |
| R^2_0 |     0.125  |     0.116777   |     0.133223  |  0.00296153 |             1 |
| R G_0 |    -0.1704 |    -0.1783     |    -0.1625    |  0.00284535 |             1 |
| R B_0 |    -0.114  |    -0.1219     |    -0.1061    |  0.00284535 |             1 |
| G^2_0 |     0.131  |     0.122777   |     0.139223  |  0.00296153 |             1 |
| G B_0 |     0.008  |     0.00010005 |     0.0158999 |  0.00284535 |             1 |
| B^2_0 |     0.137  |     0.128777   |     0.145223  |  0.00296153 |             1 |

Output_1 = Go
| var   |         value |      ci_lower |       ci_upper |          se |   significant |
|-------+---------------+---------------+----------------+-------------+---------------|
| 1_1   | -2260.67      | -2367.11      | -2154.23       | 38.3369     |             1 |
| R_1   |     1.83167   |    -0.339579  |     4.00291    |  0.782024   |             0 |
| G_1   |   198.872     |   196.7       |   201.043      |  0.782024   |             1 |
| B_1   |     9.66667   |     7.49542   |    11.8379     |  0.782024   |             1 |
| R^2_1 |     0.0547333 |     0.0369481 |     0.0725186  |  0.00640576 |             1 |
| R G_1 |    -0.0708    |    -0.0878875 |    -0.0537125  |  0.00615446 |             1 |
| R B_1 |     0.0016    |    -0.0154875 |     0.0186875  |  0.00615446 |             0 |
| G^2_1 |     0.0331333 |     0.0153481 |     0.0509186  |  0.00640576 |             1 |
| G B_1 |    -0.0256    |    -0.0426875 |    -0.00851249 |  0.00615446 |             1 |
| B^2_1 |     0.0479333 |     0.0301481 |     0.0657186  |  0.00640576 |             1 |

Output_2 = Bo
| var   |         value |      ci_lower |      ci_upper |          se |   significant |
|-------+---------------+---------------+---------------+-------------+---------------|
| 1_2   | -1980.33      | -2093.02      | -1867.65      | 40.5867     |             1 |
| R_2   |    -0.796667  |    -3.09533   |     1.502     |  0.827916   |             0 |
| G_2   |    77.5033    |    75.2047    |    79.802     |  0.827916   |             1 |
| B_2   |   207.653     |   205.355     |   209.952     |  0.827916   |             1 |
| R^2_2 |     0.0682667 |     0.0494377 |     0.0870956 |  0.00678167 |             1 |
| R G_2 |    -0.0448    |    -0.0628903 |    -0.0267097 |  0.00651562 |             1 |
| R B_2 |    -0.0312    |    -0.0492903 |    -0.0131097 |  0.00651562 |             1 |
| G^2_2 |     0.0474667 |     0.0286377 |     0.0662956 |  0.00678167 |             1 |
| G B_2 |    -0.044     |    -0.0620903 |    -0.0259097 |  0.00651562 |             1 |
| B^2_2 |     0.0674667 |     0.0486377 |     0.0862956 |  0.00678167 |             1 |
#+end_example

** Making predictions

We can use the instance in sklearn predictions.

#+BEGIN_SRC jupyter-python
sr.predict(pd.DataFrame([[200, 200, 200]], columns=['R', 'G', 'B']),
           return_std=True)
#+END_SRC

#+RESULTS:
| array | (((62199 41453.33333333 57419.66666667))) | array | (((136.59354266 295.45042372 312.78852856))) |

** Using the model for optimization

Finally, we can use the model we fit in optimization. Here we look for the inputs that result in an output of (10000, 10000, 10000).

#+BEGIN_SRC jupyter-python
from scipy.optimize import minimize

def objective(RGB):
    yp = sr.predict(pd.DataFrame([RGB], columns=['R', 'G', 'B']))
    return np.sum((yp - 10000)**2)

sol = minimize(objective, [50, 50, 50])
sol.x.astype(int)
#+END_SRC

#+RESULTS:
: array([43, 59, 35])

We can check the final answer too.

#+BEGIN_SRC jupyter-python
measure(*sol.x.astype(int))
#+END_SRC

#+RESULTS:
| 9783 | 9875 | 9903 |

That is pretty close.

*************** DONE 
CLOSED: [2024-07-08 Mon 13:14]
- [X] add parameter values in the summary report
- [X] round mae / rmse to significant figures
*************** END
